{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22063f6c-2258-43f3-9060-d62e90f6d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../clean_data/nafl/combined.large.nafl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc692fdc-0c19-4502-b168-a74018dce578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(np.sum(data['Outcome']) / data.shape[0])} of patients progressed in our dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2b82b-7186-4cc0-81cd-a4a69fe2d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b54f80-2f18-4a0c-aad8-0f1abda909d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['DaysUntilFirstProgression', 'Outcome'])\n",
    "X.set_index('StudyID', inplace=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07d30f-9168-4f87-8beb-60b8b9caa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X and Y datasets\n",
    "\n",
    "# data = data.drop(columns='DaysUntilFirstProgression')\n",
    "data = data.drop(columns='Outcome')\n",
    "data = data.drop(columns='Censored')\n",
    "\n",
    "# Y = data[['StudyID', 'Outcome']]\n",
    "Y = data[['StudyID', 'DaysUntilFirstProgression']]\n",
    "X = data.drop(columns='DaysUntilFirstProgression')\n",
    "\n",
    "X = X.set_index('StudyID')\n",
    "Y = Y.set_index('StudyID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f1414-205a-4400-aa0f-c85a42ea19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is enabled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # need to define device since python can use both cpu and gpu\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"Shape of X: {X.shape}. Shape of Y: {Y.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460c8cc-90a4-4648-af52-1b8f77a9ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to tensors\n",
    "X_numpy = X.values.astype(np.int64) # turn into a numpy array\n",
    "X_torch = torch.from_numpy(X_numpy)\n",
    "\n",
    "Y_numpy = Y.values.astype(np.int64) # turn into a numpy array\n",
    "Y_torch = torch.from_numpy(Y_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91bf07-248f-4b31-b4ce-52f77809bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 30, 90, 180, 365, 1000, 2000]\n",
    "labels = ['0-30', '31-90', '91-180', '181-365', '366-1000', '1001-2000']\n",
    "\n",
    "Y_binned = pd.cut(Y['DaysUntilFirstProgression'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "bin_counts = Y_binned.value_counts().sort_index()\n",
    "print(bin_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cca668-1912-4993-9fed-998c05d7ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Y['DaysUntilFirstProgression']) #, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e1968-21af-446d-948a-c0b0540205ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y['DaysUntilFirstProgression'], bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef28113-6382-4842-8234-08f3b8fedef8",
   "metadata": {},
   "source": [
    "# establish model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639faa9e-9384-4f21-afc5-37c32f2b8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate the dataset\n",
    "class MAFLDDataset(Dataset): # must contain init, len, and getitem\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = MAFLDDataset(X_torch, Y_torch)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True) # batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a0693-fa83-4206-bed6-e044b96232af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define by subclassing nn.Module and initialize the neural network layers in __init__.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 1), # no activation follows this layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c4015-e45f-4795-b49d-b62fe75017af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.linear_relu_stack:\n",
    "    print(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075501b8-6b30-4cab-902e-a37147c2fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = MAFLDDataset(X_train, y_train)\n",
    "train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05628bd-1efc-4826-8cf5-b9ab08fdfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 30, 90, 180, 365, 1000, 2000]\n",
    "labels = ['0-30', '31-90', '91-180', '181-365', '366-1000', '1001-2000']\n",
    "\n",
    "Y_binned = pd.cut(Y['DaysUntilFirstProgression'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "bin_counts = Y_binned.value_counts().sort_index()\n",
    "print(bin_counts)\n",
    "bins = [0, 30, 90, 180, 365, 1000, 2000]\n",
    "labels = ['0-30', '31-90', '91-180', '181-365', '366-1000', '1001-2000']\n",
    "\n",
    "\n",
    "y_train_df = pd.DataFrame({'y': y_train.cpu().numpy().squeeze()})\n",
    "y_train_binned = pd.cut(y_train_df['y'], bins=bins, labels=labels, include_lowest=True)\n",
    "bin_counts = y_train_binned.value_counts().sort_index()\n",
    "bin_weights = 1 / bin_counts\n",
    "bin_weights = bin_weights / bin_weights.sum()\n",
    "\n",
    "def get_weight_from_y(y_val):\n",
    "    for i in range(len(bins) - 1):\n",
    "        if bins[i] <= y_val <= bins[i + 1]:\n",
    "            return bin_weights[labels[i]]\n",
    "    return 1.0  # fallback in case of unexpected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cbe09-a42a-4538-9ec4-dc87b03c1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 30 epochs\n",
    "num_epochs = 30 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_data:\n",
    "        batch_X = torch.tensor(batch_X).to(device)\n",
    "        batch_y = torch.tensor(batch_y).to(device)\n",
    "        \n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # batch_weights = torch.tensor(\n",
    "        #     [get_weight_from_y(y.item()) for y in batch_y],\n",
    "        #     dtype=torch.float,\n",
    "        #     device=device\n",
    "        # )\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # weighted_loss = (loss * batch_weights).mean()\n",
    "\n",
    "        # weighted_loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460f7c0-803e-44fc-869d-12fd58da9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4f6b0-bfe8-4782-97ae-556ee0ecdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on test data\n",
    "Y_hat_test = model(X_test.float().to(device)) # run on testing data\n",
    "\n",
    "# evaluate via MSE\n",
    "print(f'MSE: {mean_squared_error(y_test, Y_hat_test.cpu().detach().numpy())}')\n",
    "print(f'Absolute Error: {mean_absolute_error(y_test, Y_hat_test.cpu().detach().numpy())}')\n",
    "\n",
    "# training loss with regular loss: 5248.9624, 1136.1140\n",
    "# training loss with weighted loss: 11919.7793\n",
    "# training loss with leakyReLU architecture: 18536.0098, 668.5651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189cb50d-3a0c-4dcd-ac86-5b389a9546fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test - Y_hat_test.cpu().detach().numpy(), bins=np.linspace(-2500,2500,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365fca6-3c52-4610-9f06-2b9c0cc6d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_hat_test.cpu().detach().numpy(), bins=np.linspace(0, 3000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8c94f-22a6-49ee-af0a-a8f3519573ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y['DaysUntilFirstProgression']) #, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372f813-c730-4ca9-94f6-f5a052896059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True distribution median: {Y['DaysUntilFirstProgression'].median()}\")\n",
    "print(f'Predicted distribution median: {np.median(Y_hat_test.cpu().detach().numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4ec4e-f601-448a-b6c5-4ca417404d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True distribution mean: {Y['DaysUntilFirstProgression'].mean()}\")\n",
    "print(f'Predicted distribution mean: {np.mean(Y_hat_test.cpu().detach().numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252b836-0e81-49a9-be14-c45fd5da56e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-nafld]",
   "language": "python",
   "name": "conda-env-torch-nafld-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cox-PH and DeepSurv\n",
    "\n",
    "In this notebook we will train the [Cox-PH method](http://jmlr.org/papers/volume20/18-424/18-424.pdf), also known as [DeepSurv](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1).\n",
    "This notebook is modified from [the Cox-PH GitHub Repo](https://github.com/havakv/pycox?tab=readme-ov-file#methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment to install `sklearn-pandas`\n",
    "! pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../clean_data/nafl/combined.large.nafl.csv\")\n",
    "\n",
    "X = data.drop(columns=['DaysUntilFirstProgression', 'Outcome'])\n",
    "Y = data[['StudyID', 'DaysUntilFirstProgression', 'Outcome']]\n",
    "\n",
    "X = X.set_index('StudyID')\n",
    "Y = Y.set_index('StudyID')\n",
    "\n",
    "# convert categorical variables to one-hot variables\n",
    "Y['Outcome'] = Y['Outcome'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the dataframe mapper method to standardize\n",
    "cols_numerical = [x for x in X.columns if any(keyword in x for keyword in ['BMI', 'Age', 'Lab'])]\n",
    "cols_other = [x for x in X.columns if x not in cols_numerical]\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_numerical]\n",
    "leave = [(col, None) for col in cols_other]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_mapper.fit_transform(X_train).astype('float32')\n",
    "x_val = x_mapper.transform(X_val).astype('float32')\n",
    "x_test = x_mapper.transform(X_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target = lambda df: (df['DaysUntilFirstProgression'].values, df['Outcome'].values)\n",
    "y_train = get_target(y_train)\n",
    "y_val = get_target(y_val)\n",
    "durations_test, events_test = get_target(y_test)\n",
    "val = x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to the correct datatypes\n",
    "y_train_new = (y_train[0].astype('float32'), y_train[1].astype('int32'))\n",
    "y_val_new = (y_val[0].astype('float32'), y_val[1].astype('int32'))\n",
    "durations_test = durations_test.astype('float32')\n",
    "events_test = events_test.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net\n",
    "\n",
    "We create a simple MLP with two hidden layers, ReLU activations, batch norm and dropout. \n",
    "Here, we just use the `torchtuples.practical.MLPVanilla` net to do this.\n",
    "\n",
    "Note that we set `out_features` to 1, and that we have not `output_bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 32]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.1\n",
    "output_bias = False\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                              dropout, output_bias=output_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model we need to define an optimizer. You can choose any `torch.optim` optimizer, but here we instead use one from `tt.optim` as it has some added functionality.\n",
    "We use the `Adam` optimizer, but instead of choosing a learning rate, we will use the scheme proposed by [Smith 2017](https://arxiv.org/pdf/1506.01186.pdf) to find a suitable learning rate with `model.lr_finder`. See [this post](https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoxPH(net, tt.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "lrfinder = model.lr_finder(x_train, y_train_new, batch_size, tolerance=10)\n",
    "_ = lrfinder.plot()\n",
    "# plt.savefig(\"batch_loss_learning_rate.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfinder.get_best_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the `EarlyStopping` callback to stop training when the validation loss stops improving. After training, this callback will also load the best performing model in terms of validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 512\n",
    "callbacks = [tt.callbacks.EarlyStopping()]\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "log = model.fit(x_train, y_train_new, batch_size, epochs, callbacks, verbose,\n",
    "                val_data=val, val_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = log.plot()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Training Epochs')\n",
    "# plt.savefig(\"loss_per_epoch.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the partial log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.partial_log_likelihood(*val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training PLL\n",
    "train = x_train, y_train_new\n",
    "model.partial_log_likelihood(*train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "For evaluation we first need to obtain survival estimates for the test set.\n",
    "This can be done with `model.predict_surv` which returns an array of survival estimates, or with `model.predict_surv_df` which returns the survival estimates as a dataframe.\n",
    "\n",
    "However, as `CoxPH` is semi-parametric, we first need to get the non-parametric baseline hazard estimates with `compute_baseline_hazards`. \n",
    "\n",
    "Note that for large datasets the `sample` argument can be used to estimate the baseline hazard on a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.compute_baseline_hazards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = model.predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_studyid = y_test.reset_index(inplace=False)\n",
    "pos_outcome = y_test_studyid.index[y_test['Outcome'] == 1].tolist()\n",
    "neg_outcome = y_test_studyid.index[y_test['Outcome'] == 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate one curve for all patients with positive progression\n",
    "pos_surv = surv[pos_outcome]\n",
    "pos_surv_avg = pos_surv.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate one curve for all patients with negative progression\n",
    "neg_surv = surv[neg_outcome]\n",
    "neg_surv_avg = neg_surv.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can use the `EvalSurv` class for evaluation the concordance, brier score and binomial log-likelihood. Setting `censor_surv='km'` means that we estimate the censoring distribution by Kaplan-Meier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.concordance_td()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "_ = ev.brier_score(time_grid).plot()\n",
    "\n",
    "# plt.figure(figsize=(6.4, 4.8))\n",
    "plt.xlabel('Time t (days)')\n",
    "plt.ylabel('Brier Score')\n",
    "plt.ylim(0, 0.9)\n",
    "plt.title('Cox PH DeepSurv Brier Score Over Time')\n",
    "# plt.savefig(\"results/deepsurv_brier_score.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_brier_score(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_nbll(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the same analysis for the training dataset\n",
    "\n",
    "surv_train = model.predict_surv_df(x_train)\n",
    "\n",
    "ev_train = EvalSurv(\n",
    "    surv_train,\n",
    "    durations=y_train_new[0],\n",
    "    events=y_train_new[1],\n",
    "    censor_surv='km'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_train.concordance_td()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_train.integrated_brier_score(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_train.integrated_nbll(time_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt to produce SHAP scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "\n",
    "net.eval()\n",
    "net.to('cpu')\n",
    "\n",
    "# DeepExplainer works directly with the PyTorch model\n",
    "explainer = shap.DeepExplainer(net, torch.tensor(x_train, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(torch.tensor(x_test, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed = shap_values.squeeze(-1)\n",
    "shap_values_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with human readable names\n",
    "features = ['Transaminase-SGOT AST Blood Test', 'Iron Binding Capacity Test', 'Chest pain', 'Essential (primary) hypertension', 'Hemoglobin Blood Test', 'Very low-density lipoprotein Blood Test', 'Type 2 diabetes mellitus', 'Mean Corpuscular Hemoglobin Blood Test', 'Abnormal results of liver function studies', 'Vitamin D Blood Test','Hematocrit Blood Test','Mean corpuscular volume Blood Test','Other specified disorders involving the immune mechanism','Most recent BMI before MASLD diagnosis','Platelet Blood Test','Cardiac Risk Ratio','Red blood cell count Blood Test','Vitamin D deficiency, unspecified','Other nonspecific abnormal finding of lung field','Other specified counseling' ]\n",
    "shap_values_top = shap_values_squeezed[:, :20]\n",
    "x_test_top = x_test[:, :20]\n",
    "# features_top = features[:20]\n",
    "features_top = X.columns[:20]\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "shap.summary_plot(shap_values_top, x_test_top, feature_names=features_top, show=False, max_display=10)\n",
    "plt.xlabel('SHAP value (impact on model output)')\n",
    "plt.title('Cox PH DeepSurv Influential Features')\n",
    "# plt.savefig('results/coxph_shap.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean absolute SHAP values per feature\n",
    "shap_mean_abs = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "# Create a Series for easy sorting\n",
    "shap_series = pd.Series(shap_mean_abs, index=X.columns)\n",
    "\n",
    "# Get top 20 feature names\n",
    "top20_features = shap_series.sort_values(ascending=False).head(20).index.tolist()\n",
    "top20_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top features according to shap score\n",
    "# rank features by mean absolute SHAP value\n",
    "feature_names = X.columns\n",
    "\n",
    "mean_abs_shap = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "top_indices = np.argsort(mean_abs_shap)[::-1]  # descending order\n",
    "\n",
    "top_features = [(feature_names[i], mean_abs_shap[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the mean SHAP value for each feature (not absolute)\n",
    "mean_shap = shap_values_squeezed.mean(axis=0)\n",
    "\n",
    "# 2. Get indices of top 10 positive and top 10 negative impact features\n",
    "top_positive_indices = np.argsort(mean_shap)[-10:]  # most positive\n",
    "top_negative_indices = np.argsort(mean_shap)[:10]   # most negative\n",
    "\n",
    "# 3. Retrieve feature names and their SHAP values\n",
    "top_positive_features = [(feature_names[i], mean_shap[i]) for i in reversed(top_positive_indices)]\n",
    "top_negative_features = [(feature_names[i], mean_shap[i]) for i in top_negative_indices]\n",
    "\n",
    "pos_names = [x[0] for x in top_positive_features]\n",
    "\n",
    "neg_names = [x[0] for x in top_negative_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot survival curves of populations stratified by certain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i can only plot the survival estimates for the x_test\n",
    "y_test_id = y_test.reset_index(inplace=False)\n",
    "pos_outcome = y_test_studyid.index[X_test_scaled['Lab_2091-7'] > 0].tolist()\n",
    "neg_outcome = y_test_studyid.index[X_test_scaled['Lab_2091-7'] <= 0].tolist()\n",
    "\n",
    "pos_surv = surv[pos_outcome]\n",
    "pos_surv_avg = pos_surv.mean(axis=1)\n",
    "\n",
    "plt.plot(pos_surv_avg, label='Patients with Positive Lab_2091-7')\n",
    "plt.plot(neg_surv_avg, label='Censored Patients')\n",
    "\n",
    "plt.ylabel('Average S(t | x)')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.title('Average Survival Prediction')\n",
    "plt.legend()\n",
    "# plt.savefig(\"average_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = X_test.index[X_test['Code_R94.5'] == True].tolist()\n",
    "\n",
    "studyid_to_col = {study_id: i for i, study_id in enumerate(X_test.index)}\n",
    "target_cols = [studyid_to_col[sid] for sid in target_ids if sid in studyid_to_col]\n",
    "pos_surv = surv.iloc[:, target_cols]\n",
    "\n",
    "pos_surv_avg = pos_surv.mean(axis=1)\n",
    "neg_surv_avg = neg_surv.mean(axis=1)\n",
    "\n",
    "plt.plot(pos_surv_avg, label='Patients Diagnosed with Abnormal Liver Function (n=609)')\n",
    "plt.plot(neg_surv_avg, label='Patients Diagnosed without Abnormal Liver Function (n=2866)')\n",
    "\n",
    "plt.ylabel('Average S(t | x)')\n",
    "_ = plt.xlabel('Time t (days)')\n",
    "plt.title('Predicted Time to Progression Stratified by Abnormal Liver Diagnosis')\n",
    "plt.legend()\n",
    "plt.savefig(\"results/coxph_abnormalliver.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pycox-env]",
   "language": "python",
   "name": "conda-env-pycox-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dd24c4-366d-40f3-9b38-0dc2f86dab55",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af61c8-50a1-4766-9abe-f5ee785769ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../clean_data/nafl/combined.large.nafl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04fdf4-22b4-482e-aa96-2aa28987015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X and Y datasets\n",
    "\n",
    "data = data.drop(columns='DaysUntilFirstProgression')\n",
    "# data = data.drop(columns='Outcome')\n",
    "data = data.drop(columns='Censored')\n",
    "\n",
    "Y = data[['StudyID', 'Outcome']]\n",
    "# Y = data[['StudyID', 'DaysUntilFirstProgression']]\n",
    "X = data.drop(columns='Outcome')\n",
    "X = data.drop(columns=['mean_BMI_category', 'last_BMI_category'])\n",
    "\n",
    "\n",
    "X = X.set_index('StudyID')\n",
    "Y = Y.set_index('StudyID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc94b3-06b4-4da0-8b64-c084f6eec226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is enabled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # need to define device since python can use both cpu and gpu\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"Shape of X: {X.shape}. Shape of Y: {Y.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee54df7-7a66-4612-8e21-ec8739e66f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to tensors\n",
    "X_numpy = X.values.astype(np.int64) # turn into a numpy array\n",
    "\n",
    "# standardize our features\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler()\n",
    "X_numpy = scaler.fit_transform(X_numpy)\n",
    "\n",
    "X_torch = torch.from_numpy(X_numpy)\n",
    "\n",
    "Y_numpy = Y.values.astype(np.int64) # turn into a numpy array\n",
    "Y_torch = torch.from_numpy(Y_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ced462-fa80-4fc4-a538-80ea20c50bee",
   "metadata": {},
   "source": [
    "### trying smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e052d83-3974-41b0-8de3-cae92698e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a0964-54de-4539-8470-299b00f6d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying smote\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "\n",
    "Y_numpy = Y.values.astype(np.int64).ravel()\n",
    "X_numpy = X.values.astype(np.float32)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numpy)\n",
    "\n",
    "# Count before SMOTE\n",
    "print(\"Class distribution before SMOTE:\", Counter(Y_numpy))\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, Y_resampled = smote.fit_resample(X_scaled, Y_numpy)\n",
    "\n",
    "# Count after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", Counter(Y_resampled))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_torch = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "Y_torch = torch.tensor(Y_resampled, dtype=torch.float32).unsqueeze(1)  # make (n_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292f029-b57a-416a-b5b6-3cad3b58095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled_df = pd.DataFrame(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cee5f-f784-4497-8519-5151df6cec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = X_resampled_df.iloc[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23565ae0-dc8f-427b-9ea5-12089e658f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = pd.DataFrame(X_scaled).iloc[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7e863-3619-4325-bb76-57ab4b906756",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(foo - bar != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fe67e-5d8a-4aba-b50d-f3f245566630",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151e843-b0a9-43ca-9518-a44af7cb4865",
   "metadata": {},
   "source": [
    "# establish the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f25b80-4e98-4d4f-9cd8-0304b0c141c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate the dataset\n",
    "class MAFLDDataset(Dataset): # must contain init, len, and getitem\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = MAFLDDataset(X_torch, Y_torch)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True) # batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306cfb8-7bc3-4d52-9b1f-1f3f32a32842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define by subclassing nn.Module and initialize the neural network layers in __init__.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f3493-bbc2-49d8-a146-4a4564571e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of NeuralNetwork, move to device, print its structure\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# define loss function and optimizer\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.BCELoss() # if using BCELoss, do not run the sigmoid layer in the forward step!\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # start with this baseline learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89f420-52c3-4984-bd56-4c5df26df910",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# run the untrained model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5bbf9-d7b0-4ea0-ac01-e082fb4ae098",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # move data to device\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Reshape labels if needed\n",
    "        # batch_y = batch_y.unsqueeze(1)  # Make sure batch_y is (batch_size, 1)\n",
    "\n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e6c25-de9f-4678-82aa-4123f9707c35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## evaluate performance on predicting binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0b2ff-5354-43da-a18f-fb1a0b74158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "X_input = torch.tensor(X_torch, device=device, dtype=torch.float32)\n",
    "Y_hat = model(X_input)\n",
    "\n",
    "predictions = (Y_hat >= 0.5).float()  # 0 if <0.5, 1 if >=0.5\n",
    "print(f'Predicted classes: {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a2aef-1392-40f7-bd10-aa0f4ad0b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance\n",
    "\n",
    "print(confusion_matrix(Y, predictions.cpu().detach().numpy()))\n",
    "print(classification_report(Y, predictions.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3a62c-4b38-4f1f-a58a-d752e77c6120",
   "metadata": {},
   "source": [
    "# train model on train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2485d-3754-4c84-aca8-0b7045cd1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = MAFLDDataset(X_train, y_train)\n",
    "train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea024895-86fe-4cfa-ac01-a37520247544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 30 epochs\n",
    "num_epochs = 30 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_data:\n",
    "        # move data to device\n",
    "        # batch_X = batch_X.to(device)\n",
    "        # batch_y = batch_y.to(device)\n",
    "        # print(batch_X)\n",
    "        batch_X = torch.tensor(batch_X).to(device)\n",
    "        batch_y = torch.tensor(batch_y).to(device)\n",
    "        \n",
    "        # Reshape labels if needed\n",
    "        # batch_y = batch_y.unsqueeze(1)  # Make sure batch_y is (batch_size, 1)\n",
    "\n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # weighted_loss = (loss * batch_weights).mean()\n",
    "\n",
    "        # weighted_loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c9540-05d3-451a-ad2d-30fa69de755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on test data\n",
    "Y_hat_test = model(X_test.float().to(device)) # run on testing data\n",
    "Y_hat_probs = torch.sigmoid(Y_hat_test)\n",
    "Y_pred_binary = (Y_hat_probs > 0.5).float()\n",
    "# evaluate via auroc\n",
    "print(roc_auc_score(y_test, Y_pred_binary.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2e78b-22b1-4a92-94d7-67d1749fe03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, Y_pred_binary.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425801f5-faa2-46e2-a55d-9a4f2c415720",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, Y_pred_binary.cpu().detach().numpy())\n",
    "df_cf = pd.DataFrame(cf, index=['True no progression', 'True progression'], columns=['Predicted no progression', 'Predicted progression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d928655-7b81-4926-891b-d75b25f10b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace543d-022a-45a3-8928-59252e68e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# categories = ['No progression', 'Progression']\n",
    "sns.heatmap(df_cf/np.sum(cf), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "# sns.heatmap(df_cf, annot=True, \n",
    "#             cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad544b-ba43-4d16-a89a-54cf85c9c7e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# tweaking model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54506305-07eb-4c9e-8f0f-2f1c9a930e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1), # no activation follows this layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b455e-5543-4aa2-8942-c9c1e7b81abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dropout, switching to LeakyReLU, adding batchnorm layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a915f-0c1c-4349-84bb-bb2b6244ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting skip connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.block(x))  # skip connection\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 256)\n",
    "\n",
    "        self.resblock1 = ResidualBlock(256)\n",
    "        self.resblock2 = ResidualBlock(256)\n",
    "        self.resblock3 = ResidualBlock(256)\n",
    "\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76f11e-99fe-49de-8f7f-546a1fbdaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an experiment manager that can test run the various edits we want to make\n",
    "from itertools import product\n",
    "\n",
    "search_space = {\n",
    "    \"hidden_sizes\": [[512, 128], [1024, 512, 128]],\n",
    "    \"activation\": [\"relu\", \"leaky_relu\"],\n",
    "    \"dropout\": [0.0, 0.2],\n",
    "    \"use_batchnorm\": [True, False],\n",
    "    \"learning_rate\": [1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "# Create list of all combinations\n",
    "all_configs = [dict(zip(search_space.keys(), values)) for values in product(*search_space.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fc606-f713-48c9-906b-30e0edd2fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_activation(name):\n",
    "    return {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(0.01),\n",
    "    }[name]\n",
    "\n",
    "class FlexibleNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, activation, dropout, use_batchnorm):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for h in hidden_sizes: # for each layer, construct linear + batchnorm + dropout\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(get_activation(activation))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            last_dim = h\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549a15a-b636-4b93-aaaa-ffc8940f248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop and evaluator\n",
    "def train_model(model, train_loader, val_loader, lr, device=\"cpu\", epochs=10):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = loss_fn(model(x).squeeze(), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                val_loss += loss_fn(model(x).squeeze(), y).item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "    return val_losses[-1]  # return final validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef94273-470d-47a9-9b8d-ca0f3581fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments\n",
    "def run_experiments(X_train, y_train, X_val, y_val):\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    results = []\n",
    "    for config in all_configs:\n",
    "        print(f\"Running config: {config}\")\n",
    "        model = FlexibleNetwork(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_sizes=config[\"hidden_sizes\"],\n",
    "            activation=config[\"activation\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            use_batchnorm=config[\"use_batchnorm\"]\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(MAFLDDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(MAFLDDataset(X_val, y_val), batch_size=64)\n",
    "\n",
    "        val_loss = train_model(model, train_loader, val_loader, lr=config[\"learning_rate\"])\n",
    "        results.append((config, val_loss))\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    return sorted(results, key=lambda x: x[1])  # sorted by val loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936d1ca-1097-41fb-9c11-eca8255d7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792c985-b5b9-4c6d-bd52-54fdbffdd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = MAFLDDataset(X_train, y_train)\n",
    "train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd5e50-23c5-4610-a1bc-54951587079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9bfe3e-2c56-4c32-84ce-5db161cbdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a073e-9fb4-440f-a407-48bb52eee8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-nafld]",
   "language": "python",
   "name": "conda-env-torch-nafld-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

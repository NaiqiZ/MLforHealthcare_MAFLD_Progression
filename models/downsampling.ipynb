{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1dc8d",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "Separate out the outcomes. Keep only mean and last BMI. \n",
    "\n",
    "Split into train and test datasets.\n",
    "\n",
    "Standardize the lab values, age at first diagnosis, and BMI (train the scaler on the training set and then use it on the test set).\n",
    "\n",
    "Convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/nobackup/users/ericason/mlhc-final-project/clean_data/nafl/combined.large.nafl.csv\", header=0, delimiter=\",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lists of important columns\n",
    "outcome_cols = [\"Outcome\", \"DaysUntilFirstProgression\"] # outcomes\n",
    "drop_cols = [\"StudyID\"] # columns to drop that are not outcome\n",
    "# columns that should be scaled later\n",
    "numerical_cols = [x for x in df.columns if (\"lab\" in x.lower()) or (\"age\" in x.lower()) or (\"bmi\" in x.lower() and \"category\" not in x.lower())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17490ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make features dataframe\n",
    "X = df.drop(columns=outcome_cols + drop_cols)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd86908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make outcome dataframe (including both linear and logistic outcomes)\n",
    "Y = df[[\"Outcome\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fa291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, Y_train shape: {Y_train.shape}, Y_test shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train\n",
    "# scale numerical columns and replace them in the original dataframe\n",
    "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols]) \n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d1169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numerical test features\n",
    "X_test_scaled = X_test\n",
    "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8113a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is enabled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # need to define device since python can use both cpu and gpu\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"Shape of X: {X.shape}. Shape of Y: {Y.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ced38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training data to tensors\n",
    "X_train_numpy = X_train_scaled.values.astype(np.float32) # turn into a numpy array\n",
    "X_train_torch = torch.from_numpy(X_train_numpy)\n",
    "\n",
    "Y_train_numpy = Y_train.values.astype(np.float32) # turn into a numpy array\n",
    "Y_train_torch = torch.from_numpy(Y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test data to tensors\n",
    "X_test_numpy = X_test_scaled.values.astype(np.float32) # turn into a numpy array\n",
    "X_test_torch = torch.from_numpy(X_test_numpy)\n",
    "\n",
    "Y_test_numpy = Y_test.values.astype(np.float32) # turn into a numpy array\n",
    "Y_test_torch = torch.from_numpy(Y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate the dataset\n",
    "class MAFLDDataset(Dataset): # must contain init, len, and getitem\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "test_dataset = MAFLDDataset(X_test_torch, Y_test_torch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63059aa4",
   "metadata": {},
   "source": [
    "## Establish Binary NN Model\n",
    "\n",
    "Model architecture and hyperparameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_EPOCHS=30\n",
    "LEARNING_RATE=1e-2\n",
    "NUM_SAMPLES = (Y_train_torch == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define by subclassing nn.Module and initialize the neural network layers in __init__.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00de984",
   "metadata": {},
   "source": [
    "## Train a Binary NN on the Downsampled Data\n",
    "Downsample the test data and train\n",
    "Repeat 10 times and average the metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, num_epochs=30, lr=1e-3):\n",
    "    # train model for 30 epochs\n",
    "    model.train()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # start with this baseline learning rate\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # send batch to device\n",
    "            batch_X = torch.tensor(batch_X).to(device)\n",
    "            batch_y = torch.tensor(batch_y).to(device)\n",
    "    \n",
    "            #initialize the gradients to zero\n",
    "            optimizer.zero_grad() \n",
    "    \n",
    "            # forward pass\n",
    "            outputs = model(batch_X)\n",
    "    \n",
    "            # compute loss\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            \n",
    "            # gradient descent and update the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed to 42\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cfad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store models\n",
    "models = []\n",
    "\n",
    "for i in range(10):\n",
    "    # downsample the 0 class\n",
    "    mask = (Y_train_torch == 0).squeeze()\n",
    "    X_0 = X_train_torch[mask,] # X with 0\n",
    "    Y_0 = Y_train_torch[mask,] # Y with 0\n",
    "\n",
    "    sample_indices = torch.randperm(X_0.size(0))[:NUM_SAMPLES]\n",
    "    X_0 = X_0[sample_indices,:]\n",
    "    Y_0 = Y_0[sample_indices,:]\n",
    "\n",
    "    X_concat_0 = torch.cat((X_0, X_train_torch[~mask,]), dim=0)\n",
    "    Y_concat_0 = torch.cat((Y_0, Y_train_torch[~mask,]), dim=0)\n",
    "\n",
    "    print(Y_concat_0.sum())\n",
    "    \n",
    "    train_dataset = MAFLDDataset(X_train_torch[sample_indices,:], Y_train_torch[sample_indices,:])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # batch size 64\n",
    "    \n",
    "    # create an instance of NeuralNetwork, move to device, print its structure\n",
    "    model = NeuralNetwork().to(device)\n",
    "    train_model(model, train_loader, NUM_EPOCHS, LEARNING_RATE)\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation losses across all models\n",
    "val_losses = []\n",
    "# auroc\n",
    "test_auroc = []\n",
    "train_auroc = []\n",
    "Y_pred_train = []\n",
    "Y_pred_test = []\n",
    "\n",
    "# separate test AUROC\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "    # train auroc\n",
    "    X_input = torch.tensor(X_train_torch, device=device, dtype=torch.float32)\n",
    "    Y_hat = model.sigmoid(model(X_input))\n",
    "    train_auroc.append(roc_auc_score(Y_train_numpy, (Y_hat.cpu().detach().numpy() >= 0.5)))\n",
    "    Y_pred_train.append(Y_hat.cpu().detach().numpy())\n",
    "    \n",
    "    # test auroc\n",
    "    X_input = torch.tensor(X_test_torch, device=device, dtype=torch.float32)\n",
    "    Y_hat = model(X_input)\n",
    "    test_auroc.append(roc_auc_score(Y_test_numpy, (Y_hat.cpu().detach().numpy() >= 0.5)))\n",
    "    Y_pred_test.append(Y_hat.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e57859",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_auroc) / len(train_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcee286",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test_auroc) / len(train_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63131022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the predictions\n",
    "Y_pred_train_numpy = np.array(Y_pred_train).mean(axis=0)\n",
    "Y_pred_test_numpy = np.array(Y_pred_test).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ee2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC of averaged predictions\")\n",
    "print(\"Train: \", roc_auc_score(Y_train_numpy, (Y_pred_train_numpy >= 0.5)))\n",
    "print(\"Test: \", roc_auc_score(Y_test_numpy, (Y_pred_test_numpy >= 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1f9a5",
   "metadata": {},
   "source": [
    "## Downsampling with Reweighting\n",
    "\n",
    "Model architecture and hyperparameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "counts = Counter(Y_train_numpy.ravel())\n",
    "total = sum(counts.values())\n",
    "# class_weights = {cls: total/count for cls, count in counts.items()}\n",
    "class_weights = {1: 50, 0: 1}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.array([class_weights[y] for y in Y_train_numpy.ravel()], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_train_torch = torch.from_numpy(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reweighted\n",
    "class MAFLDDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.w = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_reweight(model, data_loader, num_epochs=30, lr=1e-3):\n",
    "    # train model for 30 epochs\n",
    "    model.train()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # start with this baseline learning rate\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y, batch_w in data_loader:\n",
    "            # send batch to device\n",
    "            batch_X = torch.tensor(batch_X).to(device)\n",
    "            batch_y = torch.tensor(batch_y).to(device)\n",
    "            batch_w = torch.tensor(batch_w).to(device)\n",
    "    \n",
    "            #initialize the gradients to zero\n",
    "            optimizer.zero_grad() \n",
    "    \n",
    "            # forward pass\n",
    "            outputs = model(batch_X)\n",
    "            outputs = model(batch_X)\n",
    "\n",
    "            # manually reweight the loss\n",
    "            loss = (loss_fn(outputs, batch_y) * batch_w).mean()\n",
    "    \n",
    "            # compute loss\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            \n",
    "            # gradient descent and update the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store models\n",
    "models = []\n",
    "\n",
    "for i in range(10):\n",
    "    # downsample the 0 class\n",
    "    mask = (Y_train_torch == 0).squeeze()\n",
    "    X_0 = X_train_torch[mask,] # X with 0\n",
    "    Y_0 = Y_train_torch[mask,] # Y with 0\n",
    "\n",
    "    sample_indices = torch.randperm(X_0.size(0))[:NUM_SAMPLES]\n",
    "    X_0 = X_0[sample_indices,:]\n",
    "    Y_0 = Y_0[sample_indices,:]\n",
    "\n",
    "    X_concat_0 = torch.cat((X_0, X_train_torch[~mask,]), dim=0)\n",
    "    Y_concat_0 = torch.cat((Y_0, Y_train_torch[~mask,]), dim=0)\n",
    "\n",
    "    print(Y_concat_0.sum())\n",
    "    \n",
    "    train_dataset = MAFLDDataset(X_train_torch[sample_indices,:], Y_train_torch[sample_indices,:], W_train_torch[sample_indices])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # batch size 64\n",
    "    \n",
    "    # create an instance of NeuralNetwork, move to device, print its structure\n",
    "    model = NeuralNetwork().to(device)\n",
    "    train_model_reweight(model, train_loader, NUM_EPOCHS, LEARNING_RATE)\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation losses across all models\n",
    "val_losses = []\n",
    "# auroc\n",
    "test_auroc = []\n",
    "train_auroc = []\n",
    "Y_pred_train = []\n",
    "Y_pred_test = []\n",
    "\n",
    "# separate test AUROC\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "    # train auroc\n",
    "    X_input = torch.tensor(X_train_torch, device=device, dtype=torch.float32)\n",
    "    Y_hat = model.sigmoid(model(X_input))\n",
    "    train_auroc.append(roc_auc_score(Y_train_numpy, (Y_hat.cpu().detach().numpy() >= 0.5)))\n",
    "    Y_pred_train.append(Y_hat.cpu().detach().numpy())\n",
    "    \n",
    "    # test auroc\n",
    "    X_input = torch.tensor(X_test_torch, device=device, dtype=torch.float32)\n",
    "    Y_hat = model(X_input)\n",
    "    test_auroc.append(roc_auc_score(Y_test_numpy, (Y_hat.cpu().detach().numpy() >= 0.5)))\n",
    "    Y_pred_test.append(Y_hat.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_auroc) / len(train_auroc) # average auroc across 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aecf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ff95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test_auroc) / len(train_auroc) # test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the predictions\n",
    "Y_pred_train_numpy = np.array(Y_pred_train).mean(axis=0)\n",
    "Y_pred_test_numpy = np.array(Y_pred_test).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43804207",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC of averaged predictions\")\n",
    "print(\"Train: \", roc_auc_score(Y_train_numpy, (Y_pred_train_numpy >= 0.5)))\n",
    "print(\"Test: \", roc_auc_score(Y_test_numpy, (Y_pred_test_numpy >= 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd46e3-aba8-4ba1-99b6-b04b8725521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of people of each label\n",
    "print(Y_concat_0.sum())\n",
    "print(len(Y_concat_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88931f-cdcd-48e6-a787-634b09df213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train_numpy.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

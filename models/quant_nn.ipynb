{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dd24c4-366d-40f3-9b38-0dc2f86dab55",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af61c8-50a1-4766-9abe-f5ee785769ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "data = pd.read_csv(\"../clean_data/nafl/combined.large.nafl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04fdf4-22b4-482e-aa96-2aa28987015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X and Y datasets\n",
    "\n",
    "# data = data.drop(columns='DaysUntilFirstProgression')\n",
    "# data = data.drop(columns='Outcome')\n",
    "# data = data.drop(columns='Censored')\n",
    "\n",
    "Y = data[['StudyID', 'DaysUntilFirstProgression']]\n",
    "X = data.drop(columns=['DaysUntilFirstProgression', 'Outcome'])\n",
    "\n",
    "X = X.set_index('StudyID')\n",
    "Y = Y.set_index('StudyID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc94b3-06b4-4da0-8b64-c084f6eec226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is enabled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # need to define device since python can use both cpu and gpu\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"Shape of X: {X.shape}. Shape of Y: {Y.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85373d-3028-4e4b-a1f9-43b08fcb63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all features that start with Lab\n",
    "lab_feat = [feat for feat in X.columns if 'Lab' in feat]\n",
    "numerical_feat = ['mean_BMI', 'last_BMI', 'FirstNAFL.Age.90']\n",
    "numerical_feat.extend(lab_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151e843-b0a9-43ca-9518-a44af7cb4865",
   "metadata": {},
   "source": [
    "# establish the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f25b80-4e98-4d4f-9cd8-0304b0c141c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate the dataset\n",
    "class MAFLDDataset(Dataset): # must contain init, len, and getitem\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# dataset = MAFLDDataset(X_torch, Y_torch)\n",
    "# train_loader = DataLoader(dataset, batch_size=64, shuffle=True) # batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306cfb8-7bc3-4d52-9b1f-1f3f32a32842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define by subclassing nn.Module and initialize the neural network layers in __init__.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1), # no activation follows this layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f3493-bbc2-49d8-a146-4a4564571e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of NeuralNetwork, move to device, print its structure\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # start with this baseline learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89f420-52c3-4984-bd56-4c5df26df910",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# run the untrained model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5bbf9-d7b0-4ea0-ac01-e082fb4ae098",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # move data to device\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Reshape labels if needed\n",
    "        # batch_y = batch_y.unsqueeze(1)  # Make sure batch_y is (batch_size, 1)\n",
    "\n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e6c25-de9f-4678-82aa-4123f9707c35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## evaluate performance on predicting binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0b2ff-5354-43da-a18f-fb1a0b74158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "X_input = torch.tensor(X_torch, device=device, dtype=torch.float32)\n",
    "Y_hat = model(X_input)\n",
    "\n",
    "predictions = (Y_hat >= 0.5).float()  # 0 if <0.5, 1 if >=0.5\n",
    "print(f'Predicted classes: {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a2aef-1392-40f7-bd10-aa0f4ad0b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance\n",
    "\n",
    "print(confusion_matrix(Y, predictions.cpu().detach().numpy()))\n",
    "print(classification_report(Y, predictions.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b4b5f-e799-406f-998d-27111e901b52",
   "metadata": {},
   "source": [
    "# standardize the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65684efb-012d-4b78-b020-d87b430219bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6e541-85fa-40ca-9e1f-a030f5fd0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to only standardize the numerical columns and reattach to the rest of the dataframe\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def standardize_numerical(dataframe, num_feat=numerical_feat, training_set=True):\n",
    "    \"\"\"\n",
    "    dataframe: Pandas DataFrame\n",
    "\n",
    "    Returns: a processed DataFrame where the numerical features have been standardized and the categorical features remain the same.\n",
    "    \"\"\"\n",
    "    if training_set:\n",
    "        scaled = scaler.fit_transform(dataframe[num_feat])\n",
    "    else:\n",
    "        scaled = scaler.transform(dataframe[num_feat])\n",
    "        \n",
    "    scaled_df = pd.DataFrame(scaled, columns=num_feat, index=dataframe.index)\n",
    "    cat = dataframe.drop(columns=num_feat)\n",
    "    processed = pd.concat([scaled_df, cat], axis=1)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2485d-3754-4c84-aca8-0b7045cd1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize our features\n",
    "X_train_scaled = standardize_numerical(X_train, training_set=True)\n",
    "X_test_scaled = standardize_numerical(X_test, training_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df522a-fab8-4006-8d2f-50dd815ae79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into tensors and load into dataloaders\n",
    "X_numpy_train = X_train_scaled.values.astype('float32') # turn into a numpy array\n",
    "X_torch_train = torch.from_numpy(X_numpy_train)\n",
    "\n",
    "# y_numpy_train = Y.values.astype(np.int64) # turn into a numpy array\n",
    "# y_torch_train = torch.from_numpy(y_numpy_train)\n",
    "\n",
    "# train_dataset = MAFLDDataset(X_torch_train, y_torch_train)\n",
    "# train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "\n",
    "# scale the y vector as well\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.to_numpy().reshape(-1, 1))\n",
    "y_torch_train_scaled = torch.from_numpy(y_train_scaled)\n",
    "\n",
    "train_dataset_scaledy = MAFLDDataset(X_torch_train, y_torch_train_scaled)\n",
    "train_data_scaledy = DataLoader(train_dataset_scaledy, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82a3fa-2bc4-4b15-8fc8-d41c76afc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into tensors and load into dataloaders\n",
    "X_numpy_test = X_test_scaled.values.astype('float32') # turn into a numpy array\n",
    "X_torch_test = torch.from_numpy(X_numpy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3a62c-4b38-4f1f-a58a-d752e77c6120",
   "metadata": {},
   "source": [
    "# train model on train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea024895-86fe-4cfa-ac01-a37520247544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 30 epochs\n",
    "model.train()\n",
    "num_epochs = 50 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Training Epoch [{epoch+1}/{num_epochs}]')\n",
    "    for batch_X, batch_y in train_data_scaledy:\n",
    "        # move data to device\n",
    "        batch_X = torch.tensor(batch_X).to(device)\n",
    "        batch_y = torch.tensor(batch_y).to(device)\n",
    "\n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b760c9a-3bc9-4434-af1f-0a95cb89aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ac3c9-54cc-4c59-9606-6c27edf0e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c9540-05d3-451a-ad2d-30fa69de755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on test data\n",
    "model.eval()\n",
    "y_hat_test_scaled = model(X_torch_test.float().to(device)) # run on testing data\n",
    "y_hat_test_scaled = y_hat_test_scaled.cpu().detach()\n",
    "y_pred_rescaled = scaler_y.inverse_transform(y_hat_test_scaled.numpy())\n",
    "\n",
    "# evaluate via MSE\n",
    "print(mean_squared_error(y_test, y_pred_rescaled))\n",
    "# old: 242657.46875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f613a56-f8d1-4b6f-a65b-84777b4f4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_test, y_pred_rescaled))\n",
    "# old: 369.0030822753906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff63112-5248-49dc-8ab3-ddd41db76410",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a2929-c69f-4b8c-91e9-2a2a0290a7c3",
   "metadata": {},
   "source": [
    "# save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30b039-d933-45b8-9cd2-235aeabef6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"numeric_nn_scaled_x_and_y.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd2dea-97e6-4098-af17-940e46b21255",
   "metadata": {},
   "source": [
    "# shap scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0529d51-834c-444c-babc-eb46ec5c03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "def model_wrapper(array):\n",
    "    ''' Wrapper around the torch model() function to pass into shap explainer.\n",
    "        array: Pandas DataFrame\n",
    "        Returns: torch\n",
    "    '''\n",
    "    if isinstance(array, pd.DataFrame):\n",
    "        array = array.to_numpy()\n",
    "    array = torch.tensor(array.astype('float32')).to(device)\n",
    "    # array = array.to(device)\n",
    "    model.eval()\n",
    "    y_hat_test_scaled = model(array)\n",
    "    y_hat_test_scaled = y_hat_test_scaled.cpu().detach()\n",
    "    y_pred_rescaled = scaler_y.inverse_transform(y_hat_test_scaled.numpy())\n",
    "    return y_pred_rescaled\n",
    "\n",
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49bf0e-a7bd-41b8-a215-900cb57aae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, torch.tensor(X_train_scaled.to_numpy().astype(np.float32)).to(device))\n",
    "shap_values = explainer.shap_values(torch.tensor(X_test_scaled.to_numpy().astype(np.float32)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc620b-b613-4630-b45a-b797460e26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b0215-5c8b-4e33-9ea9-ea07d1cce173",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed = shap_values.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01da25-c989-4a42-840e-2dad43345e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in X.columns if \"Gender_Legal\" in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7be3b-5f0b-492c-b673-72c886682ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded_features = ['DaysUntilFirstProgression', 'Gender_Legal_Sex_Unknown-U']  # Replace with your actual feature names\n",
    "\n",
    "# Get new feature names list that excludes the specified feature\n",
    "feature_names_subset = [name for name in feature_names if name not in excluded_features]\n",
    "\n",
    "# Get the indices of the features to keep\n",
    "indices_to_keep = [i for i, name in enumerate(feature_names) if name not in excluded_features]\n",
    "\n",
    "# Slice the SHAP values array (assuming it's already squeezed to shape (n_samples, n_features))\n",
    "shap_values_subset = shap_values_squeezed[:, indices_to_keep]\n",
    "X_test_subset = X_test[feature_names_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704765f8-c67d-4c8e-b9c3-c508dca6db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.summary_plot(shap_values_subset, X_test_subset, feature_names=feature_names_subset)\n",
    "import matplotlib.pyplot as plt\n",
    "shap.summary_plot(shap_values_squeezed, X_test, feature_names=X.columns, show=False)\n",
    "plt.title('Time-to-event NN Feature Importance')\n",
    "# plt.savefig('results/shap_beeswarm_plot.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543181f-ec48-481e-8a2e-6975c96e4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c957b1-8665-4459-8e2e-7263e8b37719",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c4419-0756-4956-a7a6-80fcec3e72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with human readable names\n",
    "features = ['Reticulocyte Blood Test', \n",
    "'Prealbumin Blood Test', \n",
    "'Vitamin B12 Blood Test', \n",
    "'Alkaline Phosphatase Blood Test', \n",
    "'Encounter for immunization', \n",
    "'White Blood Cell Blood Test', \n",
    "'Cholesterol Test', \n",
    "'Iopamidol 76 \\% Intravenous Solution', \n",
    "'Calculated low-density Lipoprotein Blood Test', \n",
    "'Triglyceride Lab Test',\n",
    "'Ondansetron 8 mg Disintegrating Tablet',\n",
    "'Urine Volume Lab Test',\n",
    "'Unsaturated Iron Binding Blood Test',\n",
    "'Mean Corpuscular Hemoglobin Concentration Blood Test',\n",
    "'Hyperlipidemia, unspecified',\n",
    "'Very Low-density Lipoprotein Blood Test',\n",
    "'Iron Saturation Blood Test',\n",
    "'Flovent Hfa 110 mcg/Actuation Aerosol Inhaler',\n",
    "'Low-density Lipoprotein Blood Test',\n",
    "'Red Blood Cell Blood Test']\n",
    "shap_values_top = shap_values_squeezed[:, :20]\n",
    "x_test_top = X_test.iloc[:, :20]\n",
    "features_top = features[:20]\n",
    "# features_top = X.columns[:20]\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "shap.summary_plot(shap_values_top, x_test_top, feature_names=features_top, show=False, max_display=10)\n",
    "plt.xlabel('SHAP value (impact on model output)')\n",
    "plt.gca().tick_params(axis='y', labelsize=8)\n",
    "plt.title('Time-to-event Neural Network Influential Features')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('results/quant_nn_shap.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a86a55-9ab4-4251-b760-6de2e01c0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean absolute SHAP values per feature\n",
    "shap_mean_abs = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "# Create a Series for easy sorting\n",
    "shap_series = pd.Series(shap_mean_abs, index=X.columns)\n",
    "\n",
    "# Get top 20 feature names\n",
    "top20_features = shap_series.sort_values(ascending=False).head(20).index.tolist()\n",
    "top20_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a325f1-6b90-4351-9c79-ba2f9bc94eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 6))\n",
    "shap.summary_plot(shap_values_squeezed, X_test, plot_type=\"bar\", feature_names=feature_names, show=False)\n",
    "plt.title('Time-to-event NN Feature Importance')\n",
    "plt.xlabel('mean(|SHAP value|) (average impact on output)')\n",
    "plt.savefig('results/shap_bar_plot.png', format='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee46a2-079d-412a-a7dd-b1198c823d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.violin(shap_values_squeezed, X_test, plot_type=\"layered_violin\", feature_names=feature_names, max_display=20, show=False)\n",
    "plt.title('Time-to-event NN Feature Importance')\n",
    "plt.savefig('results/shap_violin_plot.png', format='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737444b-cb8a-4719-b5e1-c62e6dedc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4348c52-10e5-4590-8528-052a1b414a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4aa0a5-6bba-4bdc-8a82-4a298e116cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank features by mean absolute SHAP value\n",
    "# Calculate mean absolute SHAP value for each feature\n",
    "mean_abs_shap = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "# Get indices of top features\n",
    "top_indices = np.argsort(mean_abs_shap)[::-1]  # descending order\n",
    "\n",
    "# Get corresponding feature names and importance values\n",
    "top_features = [(feature_names[i], mean_abs_shap[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6c852-087d-4363-891e-06ed95ffe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd33f69-d121-4073-999e-bc10e21c709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_names = [x[0] for x in top_features[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30868aeb-ce21-459d-b6e2-8a4ed00b1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b02abe-3d83-4f37-b347-4e73b3da8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the mean SHAP value for each feature (not absolute)\n",
    "mean_shap = shap_values_squeezed.mean(axis=0)\n",
    "\n",
    "# 2. Get indices of top 10 positive and top 10 negative impact features\n",
    "top_positive_indices = np.argsort(mean_shap)[-10:]  # most positive\n",
    "top_negative_indices = np.argsort(mean_shap)[:10]   # most negative\n",
    "\n",
    "# 3. Retrieve feature names and their SHAP values\n",
    "top_positive_features = [(feature_names[i], mean_shap[i]) for i in reversed(top_positive_indices)]\n",
    "top_negative_features = [(feature_names[i], mean_shap[i]) for i in top_negative_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c14be-1d2d-4879-8cb6-c17faf3c04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_positive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13c2c9-74cd-462c-9b10-19d44d3380af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_negative_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004181b8-399c-4e10-8318-22124381c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_names = [x[0] for x in top_positive_features]\n",
    "pos_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64964bd-1d31-4907-b217-af55c3275222",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_names = [x[0] for x in top_negative_features]\n",
    "neg_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039e10d-6e59-4e68-bb22-5dd4ff968d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'results/quant_nn_shap_values.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    # Use pickle.dump to serialize and write the data\n",
    "    pickle.dump(shap_values_squeezed, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ff5e0-5ec6-449e-8586-3061e809cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'results/quant_nn_shap_values.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    shap_values_squeezed = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495df102-3375-4bc2-a08b-8d47ea5395b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank features by mean absolute SHAP value\n",
    "# Calculate mean absolute SHAP value for each feature\n",
    "mean_abs_shap = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "# Get indices of top features\n",
    "top_indices = np.argsort(mean_abs_shap)[::-1]  # descending order\n",
    "\n",
    "# Get corresponding feature names and importance values\n",
    "top_features = [(feature_names[i], mean_abs_shap[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef102e2-8e15-46ae-bffc-aed28664f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d975a0-f840-4778-ad5f-5c94ccbb6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614227b8-5656-493e-aea7-1b984725b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with human readable names\n",
    "top_labels = ['Reticulocyte Blood Test', \n",
    "'Prealbumin Blood Test', \n",
    "'Vitamin B12 Blood Test', \n",
    "'Alkaline Phosphatase Blood Test', \n",
    "'Encounter for immunization', \n",
    "'White Blood Cell Blood Test', \n",
    "'Cholesterol Test', \n",
    "'Iopamidol 76% Intravenous Solution', \n",
    "'Calculated low-density Lipoprotein Blood Test', \n",
    "'Triglyceride Lab Test',\n",
    "'Ondansetron 8 mg Disintegrating Tablet',\n",
    "'Urine Volume Lab Test',\n",
    "'Unsaturated Iron Binding Blood Test',\n",
    "'Mean Corpuscular Hemoglobin Concentration Blood Test',\n",
    "'Hyperlipidemia, unspecified',\n",
    "'Very Low-density Lipoprotein Blood Test',\n",
    "'Iron Saturation Blood Test',\n",
    "'Flovent Hfa 110 mcg/Actuation Aerosol Inhaler',\n",
    "'Low-density Lipoprotein Blood Test',\n",
    "'Red Blood Cell Blood Test']\n",
    "# shap_values_top = shap_values_squeezed[:, :20]\n",
    "# x_test_top = X_test.iloc[:, :20]\n",
    "# # features_top = features[:20]\n",
    "# features_top = X.columns[:20]\n",
    "\n",
    "# plt.figure(figsize=(18, 7))\n",
    "# shap.summary_plot(shap_values_top, x_test_top, feature_names=features_top, show=False, max_display=10)\n",
    "# plt.xlabel('SHAP value (impact on model output)')\n",
    "# plt.gca().tick_params(axis='y', labelsize=8)\n",
    "# plt.title('Time-to-event Neural Network Influential Features')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('results/quant_nn_shap.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654de56e-c44d-4c86-af1e-2b1b9e3dddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_importance = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "# Get the ordering of top features (same as used internally by SHAP)\n",
    "feature_order = np.argsort(shap_importance)[::-1]  # descending order\n",
    "max_display = 10  # SHAP default\n",
    "top_indices = feature_order[:max_display]\n",
    "top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6db857-5f04-4635-bbcf-bd1dd8babac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[:, 29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924ffcb-4e24-4ba0-9f6f-804574046821",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(\n",
    "    shap_values_squeezed[:, top_indices],\n",
    "    X_test.iloc[:, top_indices],\n",
    "    feature_names=top_labels,\n",
    "    show=False\n",
    ")\n",
    "plt.xlabel('SHAP value (impact on model output)')\n",
    "plt.gca().tick_params(axis='y', labelsize=11)\n",
    "plt.title('Time-to-event Neural Network Influential Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/quant_nn_shap.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2dd06-2a09-4d07-a564-750253141373",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## sanity check: training and testing on unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ee2f6-ec01-460f-9b7a-bc3da3d854cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125e06e-a328-4ac0-8f75-fd90f759e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MAFLDDataset(X_train, y_train)\n",
    "train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef6f4a-585f-4b86-b7dc-3bd07a40681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_data:\n",
    "        # move data to device\n",
    "        batch_X = torch.tensor(batch_X).to(device)\n",
    "        batch_y = torch.tensor(batch_y).to(device)\n",
    "\n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a4300-c807-4977-aead-79daefb6e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on test data\n",
    "Y_hat_test = model(X_test.float().to(device)) # run on testing data\n",
    "\n",
    "# evaluate via MSE\n",
    "print(mean_squared_error(y_test, Y_hat_test.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91d1be-d16c-4b56-9e5f-7a2b38194fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad544b-ba43-4d16-a89a-54cf85c9c7e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# tweaking model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54506305-07eb-4c9e-8f0f-2f1c9a930e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1), # no activation follows this layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b455e-5543-4aa2-8942-c9c1e7b81abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dropout, switching to LeakyReLU, adding batchnorm layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a915f-0c1c-4349-84bb-bb2b6244ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting skip connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.block(x))  # skip connection\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 256)\n",
    "\n",
    "        self.resblock1 = ResidualBlock(256)\n",
    "        self.resblock2 = ResidualBlock(256)\n",
    "        self.resblock3 = ResidualBlock(256)\n",
    "\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76f11e-99fe-49de-8f7f-546a1fbdaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an experiment manager that can test run the various edits we want to make\n",
    "from itertools import product\n",
    "\n",
    "search_space = {\n",
    "    \"hidden_sizes\": [[512, 128], [1024, 512, 128]],\n",
    "    \"activation\": [\"relu\", \"leaky_relu\"],\n",
    "    \"dropout\": [0.0, 0.2],\n",
    "    \"use_batchnorm\": [True, False],\n",
    "    \"learning_rate\": [1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "# Create list of all combinations\n",
    "all_configs = [dict(zip(search_space.keys(), values)) for values in product(*search_space.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fc606-f713-48c9-906b-30e0edd2fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_activation(name):\n",
    "    return {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(0.01),\n",
    "    }[name]\n",
    "\n",
    "class FlexibleNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, activation, dropout, use_batchnorm):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for h in hidden_sizes: # for each layer, construct linear + batchnorm + dropout\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(get_activation(activation))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            last_dim = h\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549a15a-b636-4b93-aaaa-ffc8940f248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop and evaluator\n",
    "def train_model(model, train_loader, val_loader, lr, device=\"cpu\", epochs=10):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = loss_fn(model(x).squeeze(), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                val_loss += loss_fn(model(x).squeeze(), y).item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "    return val_losses[-1]  # return final validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef94273-470d-47a9-9b8d-ca0f3581fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments\n",
    "def run_experiments(X_train, y_train, X_val, y_val):\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    results = []\n",
    "    for config in all_configs:\n",
    "        print(f\"Running config: {config}\")\n",
    "        model = FlexibleNetwork(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_sizes=config[\"hidden_sizes\"],\n",
    "            activation=config[\"activation\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            use_batchnorm=config[\"use_batchnorm\"]\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(MAFLDDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(MAFLDDataset(X_val, y_val), batch_size=64)\n",
    "\n",
    "        val_loss = train_model(model, train_loader, val_loader, lr=config[\"learning_rate\"])\n",
    "        results.append((config, val_loss))\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    return sorted(results, key=lambda x: x[1])  # sorted by val loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936d1ca-1097-41fb-9c11-eca8255d7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792c985-b5b9-4c6d-bd52-54fdbffdd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = MAFLDDataset(X_train, y_train)\n",
    "train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd5e50-23c5-4610-a1bc-54951587079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9bfe3e-2c56-4c32-84ce-5db161cbdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a073e-9fb4-440f-a407-48bb52eee8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-nafld]",
   "language": "python",
   "name": "conda-env-torch-nafld-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

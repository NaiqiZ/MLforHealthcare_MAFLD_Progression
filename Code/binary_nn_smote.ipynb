{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee9963e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d576390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/nobackup/users/ericason/mlhc-final-project/clean_data/nafl/combined.large.nafl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X and Y datasets\n",
    "\n",
    "data = data.drop(columns='DaysUntilFirstProgression')\n",
    "# data = data.drop(columns='Outcome')\n",
    "data = data.drop(columns='Censored')\n",
    "\n",
    "Y = data[['StudyID', 'Outcome']]\n",
    "# Y = data[['StudyID', 'DaysUntilFirstProgression']]\n",
    "X = data.drop(columns='Outcome')\n",
    "X = X.drop(columns=['mean_BMI_category', 'last_BMI_category'])\n",
    "\n",
    "\n",
    "X = X.set_index('StudyID')\n",
    "Y = Y.set_index('StudyID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is enabled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # need to define device since python can use both cpu and gpu\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"Shape of X: {X.shape}. Shape of Y: {Y.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bmi, lab, age \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0571fc4",
   "metadata": {},
   "source": [
    "# establish the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate the dataset\n",
    "class MAFLDDataset(Dataset): # must contain init, len, and getitem\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "#dataset = MAFLDDataset(X_torch, Y_torch)\n",
    "#train_loader = DataLoader(dataset, batch_size=64, shuffle=True) # batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define by subclassing nn.Module and initialize the neural network layers in __init__.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of NeuralNetwork, move to device, print its structure\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# define loss function and optimizer\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.BCELoss() # if using BCELoss, do not run the sigmoid layer in the forward step!\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # start with this baseline learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63642fd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# run the untrained model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bf4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # move data to device\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Reshape labels if needed\n",
    "        # batch_y = batch_y.unsqueeze(1)  # Make sure batch_y is (batch_size, 1)\n",
    "\n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e400d4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## evaluate performance on predicting binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "X_input = torch.tensor(X_torch, device=device, dtype=torch.float32)\n",
    "Y_hat = model(X_input)\n",
    "\n",
    "predictions = (Y_hat >= 0.5).float()  # 0 if <0.5, 1 if >=0.5\n",
    "print(f'Predicted classes: {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance\n",
    "\n",
    "print(confusion_matrix(Y, predictions.cpu().detach().numpy()))\n",
    "print(classification_report(Y, predictions.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5718cc0",
   "metadata": {},
   "source": [
    "# train model on train/test split\n",
    "### trying smote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "\n",
    "#train_dataset = MAFLDDataset(X_train, y_train)\n",
    "#train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7221244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Identify columns to scale\n",
    "cols_bmi_age = [col for col in X.columns if 'bmi' in col.lower() or 'age' in col.lower()]\n",
    "cols_lab = [col for col in X.columns if col.startswith(\"Lab_\") and pd.api.types.is_numeric_dtype(X[col])]\n",
    "cols_to_scale = cols_bmi_age + cols_lab\n",
    "\n",
    "# Apply scaling to selected columns (fit only on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
    "X_test_scaled[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "# Apply SMOTE only to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Convert to numpy arrays with proper types\n",
    "X_resampled = np.array(X_resampled).astype(np.float32)\n",
    "y_resampled = y_resampled.values.astype(np.float32) if isinstance(y_resampled, pd.DataFrame) else y_resampled.astype(np.float32)\n",
    "\n",
    "X_test_scaled = X_test_scaled.values.astype(np.float32)\n",
    "\n",
    "\n",
    "X_torch = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "Y_torch = torch.tensor(y_resampled, dtype=torch.float32)  # make (n_samples, 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Before SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"After SMOTE:\", Counter(y_resampled.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MAFLDDataset(X_torch, Y_torch)\n",
    "#test_data = MAFLDDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample, y_sample = train_data[0]\n",
    "print(\"X_sample shape:\", X_sample.shape)\n",
    "print(\"y_sample shape:\", y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f428d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2215a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 30 epochs\n",
    "num_epochs = 30 # typically between 10-50 for small datasets\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for batch_X, batch_y in train_data:\n",
    "        # move data to device\n",
    "        # batch_X = batch_X.to(device)\n",
    "        # batch_y = batch_y.to(device)\n",
    "        # print(batch_X)\n",
    "        batch_X = torch.tensor(batch_X).to(device)\n",
    "        batch_y = torch.tensor(batch_y).to(device)\n",
    "        \n",
    "        # Reshape labels if needed\n",
    "        # batch_y = batch_y.unsqueeze(1)  # Make sure batch_y is (batch_size, 1)\n",
    "\n",
    "        #initialize the gradients to zero\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs,  batch_y)\n",
    "\n",
    "        # weighted_loss = (loss * batch_weights).mean()\n",
    "\n",
    "        # weighted_loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # gradient descent and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83baa993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on test data\n",
    "X_test_tensor = torch.tensor(X_test_scaled).to(device)\n",
    "Y_hat_test = model(X_test_tensor)\n",
    "Y_hat_probs = torch.sigmoid(Y_hat_test)\n",
    "Y_pred_binary = (Y_hat_probs > 0.5).float()\n",
    "print(roc_auc_score(y_test.values.ravel(), Y_pred_binary.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb124dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to NumPy and count unique values\n",
    "pred_counts = np.unique(Y_pred_binary.cpu().detach().numpy(), return_counts=True)\n",
    "print(dict(zip(*pred_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a80aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, Y_pred_binary.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02905f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, Y_pred_binary.cpu().detach().numpy())\n",
    "df_cf = pd.DataFrame(cf, index=['True no progression', 'True progression'], columns=['Predicted no progression', 'Predicted progression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a420c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# categories = ['No progression', 'Progression']\n",
    "sns.heatmap(df_cf/np.sum(cf), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "# sns.heatmap(df_cf, annot=True, \n",
    "#             cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50eb376",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# tweaking model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e09722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1), # no activation follows this layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08641e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dropout, switching to LeakyReLU, adding batchnorm layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit init from parent class\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting skip connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.block(x))  # skip connection\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 256)\n",
    "\n",
    "        self.resblock1 = ResidualBlock(256)\n",
    "        self.resblock2 = ResidualBlock(256)\n",
    "        self.resblock3 = ResidualBlock(256)\n",
    "\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an experiment manager that can test run the various edits we want to make\n",
    "from itertools import product\n",
    "\n",
    "search_space = {\n",
    "    \"hidden_sizes\": [[512, 128], [1024, 512, 128]],\n",
    "    \"activation\": [\"relu\", \"leaky_relu\"],\n",
    "    \"dropout\": [0.0, 0.2],\n",
    "    \"use_batchnorm\": [True, False],\n",
    "    \"learning_rate\": [1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "# Create list of all combinations\n",
    "all_configs = [dict(zip(search_space.keys(), values)) for values in product(*search_space.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_activation(name):\n",
    "    return {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(0.01),\n",
    "    }[name]\n",
    "\n",
    "class FlexibleNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, activation, dropout, use_batchnorm):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for h in hidden_sizes: # for each layer, construct linear + batchnorm + dropout\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(get_activation(activation))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            last_dim = h\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e613e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop and evaluator\n",
    "def train_model(model, train_loader, val_loader, lr, device=\"cpu\", epochs=10):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = loss_fn(model(x).squeeze(), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                val_loss += loss_fn(model(x).squeeze(), y).item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "    return val_losses[-1]  # return final validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f708c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments\n",
    "def run_experiments(X_train, y_train, X_val, y_val):\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    results = []\n",
    "    for config in all_configs:\n",
    "        print(f\"Running config: {config}\")\n",
    "        model = FlexibleNetwork(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_sizes=config[\"hidden_sizes\"],\n",
    "            activation=config[\"activation\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            use_batchnorm=config[\"use_batchnorm\"]\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(MAFLDDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(MAFLDDataset(X_val, y_val), batch_size=64)\n",
    "\n",
    "        val_loss = train_model(model, train_loader, val_loader, lr=config[\"learning_rate\"])\n",
    "        results.append((config, val_loss))\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    return sorted(results, key=lambda x: x[1])  # sorted by val loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42708678",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = MAFLDDataset(X_train, y_train)\n",
    "train_data = DataLoader(train_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26010d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020faf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ceb520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
